```{r echo=FALSE, message=FALSE}
color1 = "orange"
color2 = "green"
```


```{r echo=FALSE}
### This is a function we found on the internet (http://wiki.stdout.org/rcookbook/Graphs/Multiple%20graphs%20on%20one%20page%20(ggplot2)/) to help ggplot use one graphical device for multiple plots. 

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

##### This is a function provided by you for a two sided Randomized matched pairs test#####
randtest.paired <- function(x, y, fun = mean, reps = 10000 - 1, COL = "lightblue"){ 
  data <- dats <- cbind(x, y) 
  observed <-fun(x - y) 
  results <- numeric(reps) 
  # n <- length(data[ ,1]) 
  # OR
  n <- dim(data)[1]
  for(i in 1:reps){ 
    for(j in 1:n){ 
      dats[j, ] <- sample(c(-1, 1), 1)*data[j, ]   
    } 
    results[i] <- (fun(dats[ ,1])) - (fun(dats[, 2]))  
  } 
  greater.p <- (sum(results >= observed) + 1)/(reps + 1) 
  less.p <- (sum(results <= observed) + 1)/(reps + 1) 
  two.sided.p <- (sum(abs(results) >= abs(observed)) + 1)/(reps + 1)
  temp <- c(greater.p, less.p, two.sided.p) 
  hist(results, breaks = "Scott", freq= FALSE, col = COL, xlab = "", main = "")
  curve(dnorm(x, mean(results), sd = sd(results)), add = TRUE, col = color2)
  abline(v = observed, col = color1, lwd = 2)
  names(temp)<- c("p.greater.than","p.less.than","two.sided.p") 
  return(temp) 
}
```


Homework/Test Two
========================================================
### NAMES: Tyler Davis, Susannah Hogue, Eitan Lees
*This is entirely our own work except as noted at the end of the document.*
```{r comment = NA, echo = FALSE}
Sys.time()
```


```{r message=FALSE, echo=FALSE}
require(PASWR)
require(ggplot2)
```

###

For all exercises that ask you to perform exploratory data analysis (EDA), you should plot the data (histogram, normal-quantile plots), describe the shape of the distribution (bell shaped, symmetric, skewed, etc.), and provide summary statistics (mean, standard deviation). For bootstrapping questions, use $10^4$ bootstraps, always provide plots, and describe the shape, spread, and bias of the distribution.  Make sure the $x$-axes for all bootstrap distributions label what you are bootstrapping.  All data sets for this homework are stored as `*.csv` files at [http://www1.appstate.edu/~arnholta/Data/](http://www1.appstate.edu/~arnholta/Data/)

**Problem 1:** Consider a population that has a normal distribution with mean $\mu = 36$, standard deviation $\sigma = 8$.
  * The sampling distribution of $\bar{X}$ for samples of size 200 will have what distribution, mean, and standard error?

> ANSWER: The resulting sampling distrubtion will be normal with a mean of 36 and a standard error of `r 8/sqrt(200)`

  * Use `R` to draw a random sample of size 200 from this population and store the values in an object named `nrs.200`. Conduct EDA on your sample.

```{r}
nrs.200 <- rnorm(200,36,8)
p <- ggplot(as.data.frame(nrs.200),aes(x=nrs.200),color=color1)
p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)

p2 <- ggplot(as.data.frame(nrs.200),aes(sample=nrs.200))
p2 + stat_qq()
```

> ANSWER: The mean, $\bar{X}$, is `r mean(nrs.200)` and the standard deviation, $\sigma$, is `r sd(nrs.200)`.

  * Compute the bootstrap distribution for $\bar{X}$ using the values stored in `nrs.200` and note the bootstrap mean and standard error.

```{r}
N = 10^4
boot.mean <- numeric(N)
for(i in 1:N){
  boot.mean[i] <- mean(sample(nrs.200,length(nrs.200),replace=TRUE))
}
```

> ANSWER: The bootstrapped mean, $\bar{X}^*$, is `r mean(boot.mean)` and standard error, $\sigma^*$, is `r sd(boot.mean)`

  * Compare the bootstrap distribution to the theoretical sampling distribution of $\bar{X}$.

> ANSWER: The bootstrap distribution has a bias of `r mean(boot.mean)-mean(nrs.200)`.

  * Repeat for samples of size $n = 50$ and $n=10$.  Carefully describe your observations about the effects of sample size on the bootstrap distribution.
  

```{r cache=TRUE}

nrs.50 <- rnorm(50,36,8)
p <- ggplot(as.data.frame(nrs.50),aes(x=nrs.50),color=color1)
p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)

N = 10^4
boot.mean.50 <- numeric(N)
for(i in 1:N){
  boot.mean.50[i] <- mean(sample(nrs.50,length(nrs.50),replace=TRUE))
}

nrs.10 <- rnorm(10,36,8)
p <- ggplot(as.data.frame(nrs.10),aes(x=nrs.10),color=color1)
p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)

N = 10^4
boot.mean.10 <- numeric(N)
for(i in 1:N){
  boot.mean.10[i] <- mean(sample(nrs.10,length(nrs.10),replace=TRUE))
}
```


> ANSWER: For $n = 50$ the bootstrap mean, $\bar{X}^*$, is `r mean(boot.mean.50)`, the standard error, $\sigma^*$, is `r sd(boot.mean.50)` and the bootstrap bias is, `r mean(boot.mean.50)-mean(nrs.50)`. For $n = 10$ the bootstrap mean, $\bar{X}^*$, is `r mean(boot.mean.10)`, the standard error, $\sigma^*$, is `r sd(boot.mean.10)` and the bootstrap bias is, `r mean(boot.mean.10)-mean(nrs.10)`. As the sample size increases, $\bar{X}^*$ and $\sigma^*$ approach $\bar{X}$ and $\frac{\sigma}{\sqrt{n}}$ respectivly. As the sample size increases, the density approches a normal distribution.

**Problem 2:** Investigate the bootstrap distribution of the median. Create random samples of size $n$  for various $n$ and bootstrap the median. Describe the bootstrap distribution. Change the sample sizes to 36 and 37; 200 and 201; 10,000 and 10,001. Note the similarities/dissimilarities, trends, and so on. Why does the parity of the sample size matter?  Modify the following code as needed to answer the questions. (Adjust the $x$ limits in the plots as needed.)

```{r comment = NA, cache = TRUE}
ne <- 14 # n even
no <- 15 # n odd
wwe <- rnorm(ne) # draw a random sample of size ne
wwo <- rnorm(no) # draw a random sample of size no
B <- 10^4  # If your computer is fast, use 10^5
even.boot <- numeric(B)
odd.boot <- numeric(B)
for (i in 1:B){
  x.even <- sample(wwe, ne, replace = TRUE)
  x.odd <- sample(wwo, no, replace = TRUE)
  even.boot[i] <- median(x.even)
  odd.boot[i] <- median(x.odd)
}
opar <- par(no.readonly = TRUE) # read current parameters
par(mfrow=c(2, 1)) # set figure layout
hist(even.boot, breaks = "Scott", xlim = c(-2, 2), main = "")
hist(odd.boot, breaks = "Scott", xlim = c(-2, 2), main = "")
par(opar)  # reset graphical parameters

P2.1 <- c(mean(even.boot),mean(odd.boot))
```


```{r comment = NA, cache = TRUE}
ne <- 36 # n even
no <- 37 # n odd
wwe <- rnorm(ne) # draw a random sample of size ne
wwo <- rnorm(no) # draw a random sample of size no
B <- 10^4  # If your computer is fast, use 10^5
even.boot <- numeric(B)
odd.boot <- numeric(B)
for (i in 1:B){
  x.even <- sample(wwe, ne, replace = TRUE)
  x.odd <- sample(wwo, no, replace = TRUE)
  even.boot[i] <- median(x.even)
  odd.boot[i] <- median(x.odd)
}
opar <- par(no.readonly = TRUE) # read current parameters
par(mfrow=c(2, 1)) # set figure layout
hist(even.boot, breaks = "Scott", xlim = c(-1, 1), main = "")
hist(odd.boot, breaks = "Scott", xlim = c(-1, 1), main = "")
par(opar)  # reset graphical parameters
P2.2 <- c(mean(even.boot),mean(odd.boot))
```


```{r comment = NA, cache = TRUE}
ne <- 200 # n even
no <- 201 # n odd
wwe <- rnorm(ne) # draw a random sample of size ne
wwo <- rnorm(no) # draw a random sample of size no
B <- 10^4  # If your computer is fast, use 10^5
even.boot <- numeric(B)
odd.boot <- numeric(B)
for (i in 1:B){
  x.even <- sample(wwe, ne, replace = TRUE)
  x.odd <- sample(wwo, no, replace = TRUE)
  even.boot[i] <- median(x.even)
  odd.boot[i] <- median(x.odd)
}
opar <- par(no.readonly = TRUE) # read current parameters
par(mfrow=c(2, 1)) # set figure layout
hist(even.boot, breaks = "Scott", xlim = c(-.5, .5), main = "")
hist(odd.boot, breaks = "Scott", xlim = c(-.5, .5), main = "")
par(opar)  # reset graphical parameters
P2.3 <- c(mean(even.boot),mean(odd.boot))
```


```{r comment = NA, cache = TRUE}
ne <- 10000 # n even
no <- 10001 # n odd
wwe <- rnorm(ne) # draw a random sample of size ne
wwo <- rnorm(no) # draw a random sample of size no
B <- 10^4  # If your computer is fast, use 10^5
even.boot <- numeric(B)
odd.boot <- numeric(B)
for (i in 1:B){
  x.even <- sample(wwe, ne, replace = TRUE)
  x.odd <- sample(wwo, no, replace = TRUE)
  even.boot[i] <- median(x.even)
  odd.boot[i] <- median(x.odd)
}
opar <- par(no.readonly = TRUE) # read current parameters
par(mfrow=c(2, 1)) # set figure layout
hist(even.boot, breaks = "Scott", xlim = c(-.1, .1), main = "")
hist(odd.boot, breaks = "Scott", xlim = c(-.1, .1), main = "")
par(opar)  # reset graphical parameters
P2.4 <- c(mean(even.boot),mean(odd.boot))
```

```{r}
df <- data.frame(Small=P2.1,Medium=P2.2,Large=P2.3,Extra_Large=P2.4,row.names=c("Even Boot","Odd Boot"))
df
```

> ANSWER: In general, the parity matters when it comes to medians because with an odd sample size there is an exact median while with an even number the median must be interpolated between two central indecies of data. As #################################################

**Problem 3:**  Import the data set `Bangladesh`. In addition to arsenic concentrations for 271 wells, the data set contains cobalt and chlorine concentrations.

```{r echo=FALSE, message=FALSE}
url = url("http://www1.appstate.edu/~arnholta/Data/Bangladesh.csv")
Banga <- read.csv(url)
```


  * Conduct EDA on the chlorine concentrations and describe the salient features.

> ANSWER: 

```{r}
chlorineplot <- ggplot(Banga, aes(x=Chlorine))
chlorineplot + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)

chlorineplot1 <- ggplot(Banga, aes(sample=Chlorine))
chlorineplot1 + stat_qq()
```

  * The `Chlorine` variable has some missing values. The following code will remove these entries:

> `chlorine <- subset(Bangladesh, select = Chlorine, subset = !is.na(Chlorine), drop = TRUE)`

```{r}
chlorine <- subset(Banga, select = Chlorine, subset = !is.na(Chlorine), drop = TRUE)
```


  * Bootstrap the mean.

```{r}
N = 10^4
boot.mean <- numeric(N)
for(i in 1:N){
  boot.mean[i] <- mean(sample(chlorine,length(chlorine),replace=TRUE))
}
```

> ANSWER:  The bootstrapped mean, $\bar{X}^*$, is `r mean(boot.mean)` and standard error, $\sigma^*$, is `r sd(boot.mean)`. The bootstrap distribution has a bias of `r mean(boot.mean)-mean(chlorine)`.

  * Find and interpret the 92% bootstrap percentile confidence interval.

```{r}
CI <- quantile(boot.mean, prob = c(.04, .96))
CI
```

> ANSWER: We are 92% sure that the Cholrine concertration falls between `r CI[1]` and `r CI[2]`.

  * What is the bootstrap estimate of the bias?  What percent of the bootstrap standard error does it represent?

> ANSWER: The bootstrap distribution has a bias of `r mean(boot.mean)-mean(chlorine)`, which is `r (mean(boot.mean)-mean(chlorine)/sd(boot.mean)*100)`%.


**Problem 4:**  The data set `FishMercury` contains mercury levels (parts per million) for 30 fish caught in lakes in Minnesota.

  * Create a histogram and a horizontal boxplot of the data. Place both graphs in the same graphical device.  Use the same scale on the $x$-axis for both graphs. What do you observe?

```{r echo=FALSE, message=FALSE}
url = url("http://www1.appstate.edu/~arnholta/Data/FishMercury.csv")
fish <- read.csv(url)

p <- ggplot(fish, aes(x=Mercury))
q <- ggplot(fish, aes(x=Mercury,y=Mercury))


plot1 <- p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)
plot2 <- q + geom_boxplot() + coord_flip()

multiplot(plot2,plot1,cols=1)
```

> ANSWER: The mean, $\bar{X}$, is `r mean(fish$Mercury)` and the standard deviation, $\sigma$, is `r sd(fish$Mercury)`.
  
  * Bootstrap the mean and record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
N = 10^4
boot.mean <- numeric(N)
fish1 <- fish[,1]
for(i in 1:N){
  boot.mean[i] <- mean(sample(fish1,length(fish1),replace=TRUE))
}

CI <- quantile(boot.mean, prob = c(.025, .975))
CI
```

> ANSWER: The bootstrapped mean, $\bar{X}^*$, is `r mean(boot.mean)` and the standard error, $\sigma^*$, is `r sd(boot.mean)`. The bootstrap distribution has a bias of `r mean(boot.mean)-mean(fish$Mercury)`. We are 95% sure that the Mercury levels fall between `r CI[1]` and `r CI[2]`.

  * Remove the outlier and bootstrap the mean of the remaining data.  Record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
fish2 <-fish[fish < max(fish)]
boot.mean2 <- numeric(N)
for(i in 1:N){
  boot.mean2[i] <- mean(sample(fish2,length(fish2),replace=TRUE))
}

CI2 <- quantile(boot.mean2, prob = c(.025, .975))
CI2

```

> ANSWER: The new bootstrapped mean, $\bar{X}^*$, is `r mean(boot.mean2)` and the standard error, $\sigma^*$, is `r sd(boot.mean2)`. The bootstrap distribution has a bias of `r mean(boot.mean)-mean(fish$Mercury)`. We are 95% sure that the Mercury levels fall between `r CI2[1]` and `r CI2[2]`.

  * What effect did removing the outlier have on the bootstrap distribution, in particular, the standard error?

> ANSWER: Removing the outlier out reduced our bootstrap mean, $\bar{X}^*$, by `r abs(mean(boot.mean2)-mean(boot.mean))/mean(boot.mean)*100`% and our standar error, $\sigma^*$, by `r abs(sd(boot.mean2)-sd(boot.mean))/sd(boot.mean)*100`%.

**Problem 5:**  Do chocolate and vanilla ice creams have the same number of calories? The data set `IceCream` contains calorie information for a sample of brands of chocolate and vanilla ice cream. Use the bootstrap to determine whether or not there is a difference in the mean number of calories.  Specifically, use a 90% bootstrap percentile confidence interval for the parameter of interest to support your conclusions.

  * What is the parameter of interest?


```{r}
url = url("http://www1.appstate.edu/~arnholta/Data/IceCream.csv")
cream <- read.csv(url)

c_cal <- cream$ChocolateCalories
v_cal <- cream$VanillaCalories

mean.diff <- mean(v_cal)-mean(c_cal)

df <- data.frame(Flavor = c(rep("Chocolate",times=39),rep("Vanilla",times=39)),Calories = c(c_cal,v_cal))

p1 <- ggplot(data = df, aes(x = Calories)) + geom_histogram(aes(y = ..density..), fill = color2) + facet_grid(Flavor ~ .) + geom_density(color = color1, size = 1.5)
p1
p2 <- ggplot(data = df, aes(sample = Calories, color = Flavor)) + stat_qq()
p2
######
N = 10^4
boot.mean.diff <- numeric(N)
for(i in 1:N){
  boot.mean.diff[i] <- mean(sample(v_cal,length(v_cal),replace=TRUE))-mean(sample(c_cal,length(c_cal),replace=TRUE))
}
CI <- quantile(boot.mean.diff, prob = c(.05, .95))
CI
#################### HELP! #########################
```


> ANSWER:The bootstrapped mean difference , $\bar{X}^*_v - \bar{X}^*_c$, is `r mean(boot.mean.diff)` and standard error, $\sigma^*$, is `r sd(boot.mean.diff)`. We are 90% confident that the difference in calories between Vanilla and Chocolate icecream falls between  `r CI[1]` and `r CI[2]`.

**Problem 6:**  Import the data set `FlightDelays` into `R`. Although the data are all on UA and AA flights flown in May in June of 2009, we will assume these represent a sample from a larger population of UA and AA flights flown under similar circumstances. We will consider the ratio of means of the flight delay lengths, $\mu_{UA}/\mu_{AA}$.

  * Create histograms, boxplots, and quantile-quantile plots for the flight delay times of UA and AA flights.  (Extra Credit if the graphs are done with ggplot2.)  Characterize the distribution of flight delay times for each carrier.  
  

> ANSWER: 

```{r}
url = url("http://www1.appstate.edu/~arnholta/Data/FlightDelays.csv")
delays <- read.csv(url)

DUAAA <- subset(delays, select=c(Carrier, Delay))

p1 <- ggplot(data = DUAAA, aes(x = Delay)) + geom_histogram(aes(y = ..density..), fill = color2) + facet_grid(Carrier ~ .) + geom_density(color = color1, size = 1.5)
p1
p2 <- ggplot(data = DUAAA, aes(sample = Delay, color = Carrier)) + stat_qq()
p2


p3 <- ggplot(data = DUAAA,aes(x = Carrier,y = Delay))

p3 + geom_boxplot() + coord_flip()

```

The distribution of flight delays in United Arilines and American Airlines are both exponential.
  
  * Bootstrap the mean of flight delay times for each airline separately and describe the resulting bootstrap distributions.  Place the histograms and normal quantile-quantile plots of the bootstrapped quantities in the same graphical device.  That is, place all four graphs in the same device. (Hint use: `par(mfrow=c(2, 2))`)
  

> ANSWER: 
  
```{r}
UA_delays <- DUAAA[DUAAA$Carrier=="UA",]$Delay
AA_delays <- DUAAA[DUAAA$Carrier=="AA",]$Delay

B <- 10^4
UA.boot <- numeric(B)
AA.boot <- numeric(B)
for (i in 1:B){
  x.UA <- sample(UA_delays, length(UA_delays), replace = TRUE)
  x.AA <- sample(AA_delays, length(AA_delays), replace = TRUE)
  UA.boot[i] <- mean(x.UA)
  AA.boot[i] <- mean(x.AA)
}

mean.ratio <- mean(UA.boot)/mean(AA.boot)
```

  * Bootstrap the ratio of means. Provide plots of the bootstrap distribution and describe the distribution. 

> ANSWER: 

```{r}
B <- 10^4
boot.ratio <- numeric(B)
for (i in 1:B){
  x.UA <- sample(UA_delays, length(UA_delays), replace = TRUE)
  x.AA <- sample(AA_delays, length(AA_delays), replace = TRUE)
  boot.ratio[i] <- mean(x.UA)/mean(x.AA)
}

p <- ggplot(as.data.frame(boot.ratio),aes(x=boot.ratio),color=color1)
p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)

p2 <- ggplot(as.data.frame(boot.ratio),aes(sample=boot.ratio))
p2 + stat_qq()
```

The distribution of the bootstrapped ratio of means is normal with a mean of `r mean(boot.ratio)` and a standard error of `r sd(boot.ratio)`

  * Find a 96% bootstrap percentile interval for the ratio of means. Interpret this interval making sure to write out the parameter to which the interval applies.

```{r}
CI <- quantile(boot.ratio, prob = c(0.02, 0.98))
CI
```

> ANSWER:

We are 96% certain that the ratio of the means of flight delays between United and American Airlines is between `r CI[1]` and `r CI[2]`.

  * What is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?
  
> ANSWER: The bootstrap distribution has a bias of `r mean(mean.ratio)-mean(boot.ratio)`, which is `r abs((mean(mean.ratio)-mean(boot.ratio))/sd(boot.ratio)*100)`%.

**Problem 7:**  Import the data set ` Twins` into `R`.  In a 1990 study by Suddath et al., reported in Ramsey and Schaeffer, researchers used magnetic resonance imaging to measure the volume of various regions of the brain for a sample of 15 monozygotic twins, where one twin was affected with schizophrenia and the other was unaffected. The twins were from North America and comprised eight male pairs, and seven female pairs ranging in age from 25 to 44 at the time of the study. The sizes in volume ($\text{cm}^3$) of the hippocampus are in the file called `Twins`.

```{r}
url = url("http://www1.appstate.edu/~arnholta/Data/Twins.csv")
twins <- read.csv(url)
```


  * Should the data be analyzed as matched pairs or be treated as if there were two independent samples?
  

> ANSWER: The data should be analyzed as matched pairs because 
  
  * Use appropriate graphics and summary statistics to describe the difference in brain volume for affected and unaffected twins.

```{r}
p <- ggplot(as.data.frame(twins$Difference),aes(x=twins$Difference),color=color1)
p + geom_histogram(aes(y = ..density..), fill = color2)+ geom_density(color = color1, size = 1.5)
p2 <- ggplot(as.data.frame(twins$Difference),aes(sample=twins$Difference))
p2 + stat_qq()
```

> ANSWER: The mean of the difference in brain volume for affected and unaffected twins is `r mean(twins$Difference)`. The standard deviation of the in brain volume for affected and unaffected twins is `r sd(twins$Difference)`.

  * Use the appropriate randomization test to ascertain if the difference in brain volume described in the previous bullet is the result of schizophrenia or if it could be explained as a chance difference. Report your _p_-value and summarize your conclusion.

```{r}
result <- randtest.paired(twins$Unaffected, twins$Affected, COL = color1, reps = 10000-1)
```

> ANSWER: With a p-value of `r result[3]` there is significant evidence to support the alternative hypothoses that the difference in brain volume is the result of schizophrenia.